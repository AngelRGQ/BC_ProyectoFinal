#  Agente Aut√≥nomo con Q-Learning (Taxi-v3)

![Python](https://img.shields.io/badge/Python-3.8%2B-blue)
![Gymnasium](https://img.shields.io/badge/Library-Gymnasium-green)
![Status](https://img.shields.io/badge/Status-Completed-success)

Este repositorio contiene la implementaci√≥n t√©cnica del *Grupo 2* para el Trabajo Grupal de Fin de Ciclo. El proyecto consiste en el desarrollo de un agente de *Aprendizaje por Refuerzo (Reinforcement Learning)* capaz de resolver el entorno Taxi-v3 de manera aut√≥noma y √≥ptima.

## üìã Descripci√≥n del Proyecto

El objetivo principal es demostrar la viabilidad t√©cnica de los algoritmos de IA en entornos din√°micos sin utilizar datasets est√°ticos. El agente (un taxi) debe aprender por s√≠ mismo a:
1. Navegar en una rejilla de 5x5.
2. Recoger a un pasajero en una ubicaci√≥n aleatoria (R, G, Y, B).
3. Dejarlo en su destino correcto en el menor tiempo posible.

Se ha implementado el algoritmo *Q-Learning Tabular* con una estrategia de exploraci√≥n *Epsilon-Greedy*, logrando que el agente aprenda la pol√≠tica √≥ptima mediante el sistema de recompensas y castigos.

## üõ†Ô∏è Tecnolog√≠as y Librer√≠as

El proyecto ha sido desarrollado en *Python* utilizando las siguientes herramientas:

* **[Gymnasium](https://gymnasium.farama.org/):** Para la simulaci√≥n del entorno Taxi-v3.
* *NumPy:* Para el manejo eficiente de la Q-Table y operaciones matem√°ticas.
* *Matplotlib:* Para la visualizaci√≥n de m√©tricas de desempe√±o y curvas de aprendizaje.

## üöÄ Instalaci√≥n y Ejecuci√≥n

Para replicar los resultados de este proyecto en tu m√°quina local:

1.  *Clonar el repositorio:*
    bash
    git clone [https://github.com/TU_USUARIO/taxi-qlearning-project.git](https://github.com/TU_USUARIO/taxi-qlearning-project.git)
    cd taxi-qlearning-project
    

2.  *Instalar dependencias:*
    Se recomienda usar un entorno virtual.
    bash
    pip install gymnasium numpy matplotlib
    

3.  *Ejecutar el Notebook:*
    Abre el archivo .ipynb en Jupyter Notebook, JupyterLab o Google Colab y ejecuta las celdas secuencialmente.
    bash
    jupyter notebook PR√ÅCTICA_TAXI_V3.ipynb
    

## üìä Metodolog√≠a (Q-Learning)

El agente utiliza una *Q-Table* de dimensiones 500x6 (500 estados posibles x 6 acciones). La actualizaci√≥n de los valores Q se realiza mediante la *Ecuaci√≥n de Bellman*:

$$Q(s,a) \leftarrow Q(s,a) + \alpha [R + \gamma \max Q(s',a') - Q(s,a)]$$

### Hiperpar√°metros Clave:
* *Learning Rate ($\alpha$):* 0.1
* *Discount Factor ($\gamma$):* 0.99
* *Epsilon Inicial:* 1.0 (Decaimiento hasta 0.01)
* *Episodios de Entrenamiento:* 2000

## üìà Resultados Obtenidos

Tras el entrenamiento de 2000 episodios, el agente fue sometido a una fase de evaluaci√≥n con los siguientes resultados:

| M√©trica | Agente Aleatorio | Agente Entrenado (Q-Learning) |
| :--- | :---: | :---: |
| *Tasa de √âxito* | ~5% | *100%* |
| *Pasos Promedio* | > 200 | *~12.5 (√ìptimo)* |
| *Recompensa Media* | Negativa | *Positiva* |

> *Nota:* El agente ha convergido a una soluci√≥n √≥ptima, eliminando movimientos innecesarios y penalizaciones.

## üë• Autores - Grupo 2

* *Integrante 1:* [Tu Nombre]
* *Integrante 2:* [Nombre Compa√±ero]
* *Integrante 3:* [Nombre Compa√±ero]
* *Instituci√≥n:* Escuela Superior Polit√©cnica de Chimborazo (ESPOCH)
* *Carrera:* Software

---
Este proyecto es de fines acad√©micos para la asignatura de Inteligencia Artificial.
